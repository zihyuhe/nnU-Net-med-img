
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-05-16 14:18:21.412077: do_dummy_2d_data_aug: True 
2025-05-16 14:18:21.415077: Using splits from existing split file: C:\Users\CUTY\Desktop\AOR\nnU_Base\nnUNet_preprocessed\Dataset001_MYTASK\splits_final.json 
2025-05-16 14:18:21.425078: The split file contains 5 splits. 
2025-05-16 14:18:21.435081: Desired fold for training: 1 
2025-05-16 14:18:21.442080: This split has 240 training and 60 validation cases. 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 1, 'patch_size': [56, 320, 256], 'median_image_size_in_voxels': [94.0, 512.0, 512.0], 'spacing': [5.0, 0.68359375, 0.68359375], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_MYTASK', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [5.0, 0.68359375, 0.68359375], 'original_median_shape_after_transp': [94, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1194.0, 'mean': 35.515421415421415, 'median': 42.0, 'min': -1022.0, 'percentile_00_5': -111.0, 'percentile_99_5': 141.0, 'std': 54.29036316376703}}} 
 
2025-05-16 14:18:34.738006: Unable to plot network architecture: 
2025-05-16 14:18:34.755565: No module named 'hiddenlayer' 
2025-05-16 14:18:34.814349:  
2025-05-16 14:18:34.837350: Epoch 0 
2025-05-16 14:18:34.859351: Current learning rate: 0.01 
2025-05-16 14:25:59.556670: train_loss 0.0137 
2025-05-16 14:25:59.556670: val_loss -0.0465 
2025-05-16 14:25:59.578671: Pseudo dice [np.float32(0.0)] 
2025-05-16 14:25:59.586671: Epoch time: 444.75 s 
2025-05-16 14:25:59.595791: Yayy! New best EMA pseudo Dice: 0.0 
2025-05-16 14:26:05.086658:  
2025-05-16 14:26:05.097168: Epoch 1 
2025-05-16 14:26:05.107169: Current learning rate: 0.00999 
2025-05-16 14:33:03.372887: train_loss -0.0554 
2025-05-16 14:33:03.373886: val_loss -0.0552 
2025-05-16 14:33:03.389886: Pseudo dice [np.float32(0.0)] 
2025-05-16 14:33:03.397886: Epoch time: 418.29 s 
2025-05-16 14:33:04.319906:  
2025-05-16 14:33:04.330908: Epoch 2 
2025-05-16 14:33:04.338908: Current learning rate: 0.00998 
2025-05-16 14:39:56.321798: train_loss -0.0537 
2025-05-16 14:39:56.322798: val_loss -0.0663 
2025-05-16 14:39:56.332799: Pseudo dice [np.float32(0.0)] 
2025-05-16 14:39:56.341799: Epoch time: 412.0 s 
2025-05-16 14:39:57.278648:  
2025-05-16 14:39:57.288656: Epoch 3 
2025-05-16 14:39:57.296510: Current learning rate: 0.00997 
2025-05-16 14:46:50.385144: train_loss -0.0593 
2025-05-16 14:46:50.386194: val_loss -0.0594 
2025-05-16 14:46:50.396159: Pseudo dice [np.float32(0.0)] 
2025-05-16 14:46:50.404005: Epoch time: 413.11 s 
2025-05-16 14:46:51.308284:  
2025-05-16 14:46:51.318063: Epoch 4 
2025-05-16 14:46:51.325206: Current learning rate: 0.00996 
2025-05-16 14:53:44.747753: train_loss -0.0603 
2025-05-16 14:53:44.747753: val_loss -0.0571 
2025-05-16 14:53:44.758754: Pseudo dice [np.float32(0.0)] 
2025-05-16 14:53:44.766754: Epoch time: 413.44 s 
2025-05-16 14:53:45.720193:  
2025-05-16 14:53:45.729971: Epoch 5 
2025-05-16 14:53:45.738207: Current learning rate: 0.00995 
2025-05-16 15:00:40.650321: train_loss -0.0608 
2025-05-16 15:00:40.650837: val_loss -0.0698 
2025-05-16 15:00:40.661836: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:00:40.669527: Epoch time: 414.93 s 
2025-05-16 15:00:41.557631:  
2025-05-16 15:00:41.567636: Epoch 6 
2025-05-16 15:00:41.576003: Current learning rate: 0.00995 
2025-05-16 15:07:35.877236: train_loss -0.0655 
2025-05-16 15:07:35.887329: val_loss -0.068 
2025-05-16 15:07:35.896408: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:07:35.906017: Epoch time: 414.32 s 
2025-05-16 15:07:36.871663:  
2025-05-16 15:07:36.888622: Epoch 7 
2025-05-16 15:07:36.897769: Current learning rate: 0.00994 
2025-05-16 15:14:30.876075: train_loss -0.0669 
2025-05-16 15:14:30.887664: val_loss -0.0652 
2025-05-16 15:14:30.896665: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:14:30.904664: Epoch time: 414.01 s 
2025-05-16 15:14:32.536978:  
2025-05-16 15:14:32.546956: Epoch 8 
2025-05-16 15:14:32.555970: Current learning rate: 0.00993 
2025-05-16 15:21:27.841547: train_loss -0.0681 
2025-05-16 15:21:27.852873: val_loss -0.0683 
2025-05-16 15:21:27.860775: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:21:27.869947: Epoch time: 415.31 s 
2025-05-16 15:21:28.783105:  
2025-05-16 15:21:28.793120: Epoch 9 
2025-05-16 15:21:28.802561: Current learning rate: 0.00992 
2025-05-16 15:28:24.478680: train_loss -0.0751 
2025-05-16 15:28:24.490835: val_loss -0.0773 
2025-05-16 15:28:24.499861: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:28:24.508488: Epoch time: 415.7 s 
2025-05-16 15:28:25.382351:  
2025-05-16 15:28:25.392325: Epoch 10 
2025-05-16 15:28:25.400689: Current learning rate: 0.00991 
2025-05-16 15:35:18.504230: train_loss -0.0765 
2025-05-16 15:35:18.516937: val_loss -0.0657 
2025-05-16 15:35:18.525895: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:35:18.533949: Epoch time: 413.12 s 
2025-05-16 15:35:19.452317:  
2025-05-16 15:35:19.463478: Epoch 11 
2025-05-16 15:35:19.472467: Current learning rate: 0.0099 
2025-05-16 15:42:12.842826: train_loss -0.0629 
2025-05-16 15:42:12.855669: val_loss -0.0725 
2025-05-16 15:42:12.865284: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:42:12.874370: Epoch time: 413.39 s 
2025-05-16 15:42:13.764621:  
2025-05-16 15:42:13.774344: Epoch 12 
2025-05-16 15:42:13.782995: Current learning rate: 0.00989 
2025-05-16 15:49:09.463392: train_loss -0.0754 
2025-05-16 15:49:09.473274: val_loss -0.0541 
2025-05-16 15:49:09.482861: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:49:09.491516: Epoch time: 415.7 s 
2025-05-16 15:49:10.386203:  
2025-05-16 15:49:10.395261: Epoch 13 
2025-05-16 15:49:10.402708: Current learning rate: 0.00988 
2025-05-16 15:56:06.218627: train_loss -0.057 
2025-05-16 15:56:06.228827: val_loss -0.0638 
2025-05-16 15:56:06.237710: Pseudo dice [np.float32(0.0)] 
2025-05-16 15:56:06.246623: Epoch time: 415.84 s 
2025-05-16 15:56:07.164578:  
2025-05-16 15:56:07.174858: Epoch 14 
2025-05-16 15:56:07.182896: Current learning rate: 0.00987 
2025-05-16 16:03:02.670849: train_loss -0.0538 
2025-05-16 16:03:02.683146: val_loss -0.0638 
2025-05-16 16:03:02.691604: Pseudo dice [np.float32(0.0)] 
2025-05-16 16:03:02.700136: Epoch time: 415.51 s 
2025-05-16 16:03:03.610683:  
2025-05-16 16:03:03.619736: Epoch 15 
2025-05-16 16:03:03.628233: Current learning rate: 0.00986 
2025-05-16 16:09:58.983995: train_loss -0.0591 
2025-05-16 16:09:58.994077: val_loss -0.0686 
2025-05-16 16:09:59.002548: Pseudo dice [np.float32(0.0)] 
2025-05-16 16:09:59.011639: Epoch time: 415.38 s 
2025-05-16 16:09:59.947713:  
2025-05-16 16:09:59.957796: Epoch 16 
2025-05-16 16:09:59.966311: Current learning rate: 0.00986 
2025-05-16 16:16:53.087822: train_loss -0.0746 
2025-05-16 16:16:53.099571: val_loss -0.0746 
2025-05-16 16:16:53.108057: Pseudo dice [np.float32(0.0587)] 
2025-05-16 16:16:53.117125: Epoch time: 413.14 s 
2025-05-16 16:16:53.125617: Yayy! New best EMA pseudo Dice: 0.005900000222027302 
2025-05-16 16:16:56.588392:  
2025-05-16 16:16:56.599599: Epoch 17 
2025-05-16 16:16:56.606495: Current learning rate: 0.00985 
2025-05-16 16:23:51.244863: train_loss -0.075 
2025-05-16 16:23:51.245382: val_loss -0.088 
2025-05-16 16:23:51.258062: Pseudo dice [np.float32(0.0851)] 
2025-05-16 16:23:51.267548: Epoch time: 414.66 s 
2025-05-16 16:23:51.275421: Yayy! New best EMA pseudo Dice: 0.013799999840557575 
2025-05-16 16:23:54.846954:  
2025-05-16 16:23:54.856913: Epoch 18 
2025-05-16 16:23:54.865880: Current learning rate: 0.00984 
2025-05-16 16:30:48.792238: train_loss -0.0744 
2025-05-16 16:30:48.803421: val_loss -0.1038 
2025-05-16 16:30:48.811448: Pseudo dice [np.float32(0.1499)] 
2025-05-16 16:30:48.820465: Epoch time: 413.95 s 
2025-05-16 16:30:48.828463: Yayy! New best EMA pseudo Dice: 0.027400000020861626 
2025-05-16 16:30:51.665320:  
2025-05-16 16:30:51.674842: Epoch 19 
2025-05-16 16:30:51.683659: Current learning rate: 0.00983 
2025-05-16 16:37:45.837460: train_loss -0.0773 
2025-05-16 16:37:45.849237: val_loss -0.0858 
2025-05-16 16:37:45.858068: Pseudo dice [np.float32(0.0581)] 
2025-05-16 16:37:45.866310: Epoch time: 414.17 s 
2025-05-16 16:37:45.874057: Yayy! New best EMA pseudo Dice: 0.030500000342726707 
2025-05-16 16:37:49.766839:  
2025-05-16 16:37:49.776912: Epoch 20 
2025-05-16 16:37:49.785896: Current learning rate: 0.00982 
2025-05-16 16:44:43.195807: train_loss -0.0817 
2025-05-16 16:44:43.205806: val_loss -0.081 
2025-05-16 16:44:43.214805: Pseudo dice [np.float32(0.0552)] 
2025-05-16 16:44:43.222923: Epoch time: 413.43 s 
2025-05-16 16:44:43.231569: Yayy! New best EMA pseudo Dice: 0.03290000185370445 
2025-05-16 16:44:46.556247:  
2025-05-16 16:44:46.565693: Epoch 21 
2025-05-16 16:44:46.573869: Current learning rate: 0.00981 
2025-05-16 16:51:40.262423: train_loss -0.0806 
2025-05-16 16:51:40.272441: val_loss -0.0675 
2025-05-16 16:51:40.280457: Pseudo dice [np.float32(0.0345)] 
2025-05-16 16:51:40.288455: Epoch time: 413.71 s 
2025-05-16 16:51:40.296457: Yayy! New best EMA pseudo Dice: 0.03310000151395798 
2025-05-16 16:51:43.776043:  
2025-05-16 16:51:43.785053: Epoch 22 
2025-05-16 16:51:43.793726: Current learning rate: 0.0098 
2025-05-16 16:58:40.297277: train_loss -0.0813 
2025-05-16 16:58:40.307148: val_loss -0.061 
2025-05-16 16:58:40.316674: Pseudo dice [np.float32(0.0)] 
2025-05-16 16:58:40.325139: Epoch time: 416.52 s 
2025-05-16 16:58:41.231774:  
2025-05-16 16:58:41.241215: Epoch 23 
2025-05-16 16:58:41.250147: Current learning rate: 0.00979 
2025-05-16 17:05:35.482573: train_loss -0.0601 
2025-05-16 17:05:35.492566: val_loss -0.0571 
2025-05-16 17:05:35.501586: Pseudo dice [np.float32(0.0)] 
2025-05-16 17:05:35.509995: Epoch time: 414.25 s 
2025-05-16 17:05:36.394885:  
2025-05-16 17:05:36.404346: Epoch 24 
2025-05-16 17:05:36.411374: Current learning rate: 0.00978 
2025-05-16 17:12:30.824091: train_loss -0.06 
2025-05-16 17:12:30.835299: val_loss -0.0801 
2025-05-16 17:12:30.843929: Pseudo dice [np.float32(0.0332)] 
2025-05-16 17:12:30.852679: Epoch time: 414.43 s 
2025-05-16 17:12:31.754369:  
2025-05-16 17:12:31.763873: Epoch 25 
2025-05-16 17:12:31.772854: Current learning rate: 0.00977 
2025-05-16 17:19:24.915021: train_loss -0.0734 
2025-05-16 17:19:24.927228: val_loss -0.0832 
2025-05-16 17:19:24.935951: Pseudo dice [np.float32(0.1213)] 
2025-05-16 17:19:24.944664: Epoch time: 413.16 s 
2025-05-16 17:19:24.953691: Yayy! New best EMA pseudo Dice: 0.03680000081658363 
2025-05-16 17:19:28.558854:  
2025-05-16 17:19:28.568014: Epoch 26 
2025-05-16 17:19:28.576725: Current learning rate: 0.00977 
2025-05-16 17:26:25.516237: train_loss -0.0668 
2025-05-16 17:26:25.517238: val_loss -0.0777 
2025-05-16 17:26:25.528238: Pseudo dice [np.float32(0.0328)] 
2025-05-16 17:26:25.537237: Epoch time: 416.96 s 
2025-05-16 17:26:26.644376:  
2025-05-16 17:26:26.654684: Epoch 27 
2025-05-16 17:26:26.663259: Current learning rate: 0.00976 
2025-05-16 17:33:24.668205: train_loss -0.0742 
2025-05-16 17:33:24.668760: val_loss -0.0992 
2025-05-16 17:33:24.678170: Pseudo dice [np.float32(0.0756)] 
2025-05-16 17:33:24.688162: Epoch time: 418.03 s 
2025-05-16 17:33:24.697156: Yayy! New best EMA pseudo Dice: 0.040300000458955765 
2025-05-16 17:33:28.207515:  
2025-05-16 17:33:28.217506: Epoch 28 
2025-05-16 17:33:28.226460: Current learning rate: 0.00975 
2025-05-16 17:40:15.094173: train_loss -0.0834 
2025-05-16 17:40:15.104173: val_loss -0.0951 
2025-05-16 17:40:15.112174: Pseudo dice [np.float32(0.0793)] 
2025-05-16 17:40:15.121173: Epoch time: 406.89 s 
2025-05-16 17:40:15.129174: Yayy! New best EMA pseudo Dice: 0.044199999421834946 
2025-05-16 17:40:18.798356:  
2025-05-16 17:40:18.808392: Epoch 29 
2025-05-16 17:40:18.817340: Current learning rate: 0.00974 
2025-05-16 17:47:02.007930: train_loss -0.0889 
2025-05-16 17:47:02.008930: val_loss -0.0955 
2025-05-16 17:47:02.019930: Pseudo dice [np.float32(0.0545)] 
2025-05-16 17:47:02.027942: Epoch time: 403.21 s 
2025-05-16 17:47:02.037605: Yayy! New best EMA pseudo Dice: 0.04529999941587448 
2025-05-16 17:47:05.068383:  
2025-05-16 17:47:05.077384: Epoch 30 
2025-05-16 17:47:05.085384: Current learning rate: 0.00973 
2025-05-16 17:53:48.115562: train_loss -0.079 
2025-05-16 17:53:48.115562: val_loss -0.0714 
2025-05-16 17:53:48.127564: Pseudo dice [np.float32(0.0)] 
2025-05-16 17:53:48.136562: Epoch time: 403.05 s 
2025-05-16 17:53:49.059278:  
2025-05-16 17:53:49.069279: Epoch 31 
2025-05-16 17:53:49.077278: Current learning rate: 0.00972 
2025-05-16 18:00:30.011559: train_loss -0.0592 
2025-05-16 18:00:30.011559: val_loss -0.0666 
2025-05-16 18:00:30.021561: Pseudo dice [np.float32(0.0)] 
2025-05-16 18:00:30.028559: Epoch time: 400.96 s 
2025-05-16 18:00:30.957172:  
2025-05-16 18:00:30.966164: Epoch 32 
2025-05-16 18:00:30.974665: Current learning rate: 0.00971 
2025-05-16 18:07:19.866278: train_loss -0.0545 
2025-05-16 18:07:19.875092: val_loss -0.0585 
2025-05-16 18:07:19.883052: Pseudo dice [np.float32(0.0)] 
2025-05-16 18:07:19.890078: Epoch time: 408.91 s 
2025-05-16 18:07:21.382599:  
2025-05-16 18:07:21.391667: Epoch 33 
2025-05-16 18:07:21.400721: Current learning rate: 0.0097 
